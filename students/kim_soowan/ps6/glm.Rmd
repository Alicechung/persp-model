---
title: 'Problem set #6: Generalized linear models'
author: "Soo Wan Kim"
date: "February 18, 2017"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(modelr)
library(broom)
library(pander)
library(pROC)
library(knitr)

options(na.action = na.warn)
set.seed(1234) #set seed
theme_set(theme_bw()) #set theme for all plots

#import data
gss <- read.csv("data/gss2006.csv") 
mhealth <- read.csv("data/mental_health.csv")
```

# Part 1: Modeling voter turnout
## Describe the data (1 point)

  1. **Plot a histogram of voter turnout. Make sure to give the graph a title and proper $x$ and $y$-axis labels. What is the unconditional probability of a given individual turning out to vote?**
  
```{r vote96_histogram}
ggplot(data = mhealth, mapping = aes(x = vote96)) +
  geom_histogram(aes(y=..count../sum(..count..))) + 
  scale_x_continuous(breaks = c(0,1), labels = c(0,1)) +
  scale_y_continuous(labels = scales::percent) + 
  labs(title = "Voter turnout for 1996 presidential election among survey respondents",
       x = "Voted (0 = No, 1 = Yes)",
       y = "Percentage",
       caption = "Source: 1998 General Social Survey") + 
  theme(plot.caption = element_text(face =  "italic"))

```
The unconditional probability of a given individual turning out to vote is `r mean(mhealth$vote96, na.rm = TRUE)`.

  2. **Generate a scatterplot of the relationship between mental health and observed voter turnout and overlay a linear smoothing line. What information does this tell us? What is problematic about this linear smoothing line?**
  
```{r vote96_scatterplot}
ggplot(data = mhealth, mapping = aes(x = mhealth_sum, y = vote96)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  scale_y_continuous(breaks = c(0,1), labels = c(0,1)) +
  labs(title = "Mental health vs. Voter turnout",
       x = "Mental health index",
       y = "Voted (0 = No, 1 = Yes)",
       caption = "Source: 1998 General Social Survey") + 
  theme(plot.caption = element_text(face =  "italic"))

```

Depression has a negative correlation with voter turnout. The problem with the linear smoothing line, however is that `vote96` is a categorical variable with values 0 and 1, not a continuous variable. Thus, the relationship between the two variables cannot be linear.

##Basic model (3 points)

**Estimate a logistic regression model of the relationship between mental health and voter turnout.**

```{r glm1_model}
glm_sing <- glm(vote96 ~ mhealth_sum, data = mhealth,
            family = binomial)
pander(summary(glm_sing))
```

  1. **Is the relationship between mental health and voter turnout statistically and/or substantively significant?**

The relationship between mental health and voter turnout is highly statistically significant, with a p-value less than 0.001. It is difficult to tell whether the relationship is substantively significant because the estimates are in log-odds.  

  2. **Interpret the estimated parameter for mental health in terms of log-odds. Generate a graph of the relationship between mental health and the log-odds of voter turnout.**

For every unit increase in `mhealth_sum`, the log-odds of voting for the 1996 election decreases by `r summary(glm_sing)$coefficients[2]`.

```{r glm1_log_odds}
mhealth_grid <- mhealth %>%
  data_grid(mhealth_sum) %>%
  add_predictions(glm_sing)

ggplot(data = mhealth_grid, mapping = aes(x = mhealth_sum, y = pred)) + 
  geom_line() + 
  labs(title = "Mental health vs. log-odds of voting for the 1996 election",
       subtitle = "vote96 ~ mhealth_sum",
       x = "Mental health index",
       y = "Log-odds of voting") + 
  theme(plot.subtitle = element_text(face = "italic"))
```

  3. **Interpret the estimated parameter for mental health in terms of odds. Generate a graph of the relationship between mental health and the odds of voter turnout.**

For every unit increase in `mhealth_sum`, the log of the odds of voting for the 1996 election decreases by `r summary(glm_sing)$coefficients[2]`.

```{r glm1_odds}
logit2odds <- function(x){
  exp(x)
}

mhealth_grid <- mhealth_grid %>%
  mutate(odds = logit2odds(pred))

ggplot(data = mhealth_grid, mapping = aes(x = mhealth_sum, y = odds)) + 
  geom_line() + 
  labs(title = "Mental health vs. odds of voting for the 1996 election",
       subtitle = "vote96 ~ mhealth_sum",
       x = "Mental health index",
       y = "Odds of voting") + 
  theme(plot.subtitle = element_text(face = "italic"))
```


  4. **Interpret the estimated parameter for mental health in terms of probabilities. Generate a graph of the relationship between mental health and the probability of voter turnout. What is the first difference for an increase in the mental health index from 1 to 2? What about for 5 to 6?**
  
For every unit increase in `mhealth_sum`, $\frac{e^x}{(1 + e^x)}$ of probablity x of voting for the 1996 election decreases by `r summary(glm_sing)$coefficients[2]`.

```{r glm1_probabilities}
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

mhealth_grid <- mhealth_grid %>%
  mutate(prob = logit2prob(pred))

ggplot(data = mhealth_grid, mapping = aes(x = mhealth_sum, y = prob)) + 
  geom_line() + 
  labs(title = "Mental health vs. probability of voting for the 1996 election",
       subtitle = "vote96 ~ mhealth_sum",
       x = "Mental health index",
       y = "Probability of voting") + 
  theme(plot.subtitle = element_text(face = "italic"))
```

```{r glm1first_difference}
#first differences in probability of voting for increase in mental health index

#1 to 2
fd_12 <- mhealth_grid$prob[mhealth_grid$mhealth_sum == 1] - mhealth_grid$prob[mhealth_grid$mhealth_sum == 2]
#5 to 6
fd_56 <- mhealth_grid$prob[mhealth_grid$mhealth_sum == 5] - mhealth_grid$prob[mhealth_grid$mhealth_sum == 6]
```

The first difference for an increase in the mental health index from 1 to 2 is `r fd_12`. The first difference for an increase from 5 to 6 is `r fd_56`.

  5. **Estimate the accuracy rate, proportional reduction in error (PRE), and the AUC for this model. Do you consider it to be a good model?**
  
```{r glm1_performance}
#estimate accuracy rate

mh_accuracy <- mhealth %>%
  add_predictions(glm_sing) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))

acc_rate <- mean(mh_accuracy$vote96 == mh_accuracy$pred, na.rm = TRUE)

#estimate PRE

# function to calculate PRE for a logistic regression model
PRE <- function(model){
  # get the actual values for y from the data
  y <- model$y
  
  # get the predicted values for y from the model
  y.hat <- round(model$fitted.values)
  
  # calculate the errors for the null model and your model
  E1 <- sum(y != median(y))
  E2 <- sum(y != y.hat)
  
  # calculate the proportional reduction in error
  PRE <- (E1 - E2) / E1
  return(PRE)
}

pre <- PRE(glm_sing)

#estimate AUC

auc_mh <- auc(mh_accuracy$vote96, mh_accuracy$prob)
```

For the `mhealth_sum`-only model, the accuracy rate is `r acc_rate`, the proportional reduction in error is `r pre`%, and the AUC is `r auc_mh`. This model works better than a random coin flip (50-50 chance of voting). On the other hand, it just barely outperforms the useless classifier, as the error is reduced by only `r pre`%. I would say this is a poor model.

##Multiple variable model (3 points)

**Using the other variables in the dataset, derive and estimate a multiple variable logistic regression model of voter turnout.**

To derive the linear predictor, I plot the relationship between voting and various predictors in the exploratory graphs below:
```{r glm2_exp_plots}
#Survival rate by age
vote_mhealth <- mhealth %>%
  group_by(mhealth_sum) %>%
  summarize(vote_mean = mean(vote96, na.rm = TRUE))

vote_age <- mhealth %>%
  group_by(age) %>%
  summarize(vote_mean = mean(vote96, na.rm = TRUE))

vote_educ <- mhealth %>%
  group_by(educ) %>%
  summarize(vote_mean = mean(vote96, na.rm = TRUE))

vote_inc10 <- mhealth %>%
  group_by(inc10) %>%
  summarize(vote_mean = mean(vote96, na.rm = TRUE))

mhealth_plot <- ggplot(data = vote_mhealth, mapping = aes(x = mhealth_sum, y = vote_mean)) + 
  geom_point() + 
  geom_smooth() + 
  ylim(0,1) + 
  scale_x_continuous(breaks = seq(0,16,1)) + 
  labs(title = "Mental health index vs. voter turnout",
       x = "Mental health index", 
       y = "Voting rate")

age_plot <- ggplot(data = vote_age, mapping = aes(x = age, y = vote_mean)) + 
  geom_point() + 
  geom_smooth() + 
  ylim(0,1) + 
  labs(title = "Age vs. voter turnout",
       x = "Age", 
       y = "Voting rate")

educ_plot <- ggplot(data = vote_educ, mapping = aes(x = educ, y = vote_mean)) + 
  geom_point() + 
  geom_smooth() + 
  ylim(0,1) + 
  labs(title = "Education vs. voter turnout",
       x = "Years of education", 
       y = "Voting rate")

inc10_plot <- ggplot(data = vote_inc10, mapping = aes(x = inc10, y = vote_mean)) + 
  geom_point() + 
  geom_smooth() + 
  ylim(0,1) + 
  labs(title = "Family income vs. voter turnout",
       x = "Family income in $10,000s", 
       y = "Voting rate")
```

```{r glm2_mhealth_plot}
mhealth_plot
```
The relationship between voting rate and the mental health score appears to follow a downward sloping line at first, then become positive at the score of 8. However, the pattern is likely due to the low frequency of higher scores, as shown below. For the multivariate logistic model I assume that the relationship is a linear one.

```{r}
vote_mhealth_na <- mhealth %>%
  filter(!is.na(vote96) & !is.na(mhealth_sum)) %>%
  group_by(mhealth_sum) %>%
  summarize(count = n())

vote_mhealth_na %>%
  kable(caption = "Frequency table, mhealth_sum\n(NA values for vote96 removed",
        col.names = c("mhealth_sum", "Count"),
        format = "html")
```

```{r glm2_age_plot}
age_plot
```

The relationship between age and voting rate appears to be parabolic, not linear.

```{r glm2_educ_plot}
educ_plot

vote_educ_na <- mhealth %>%
  filter(!is.na(vote96) & !is.na(educ)) %>%
  group_by(educ) %>%
  summarize(count = n())

vote_educ_na %>%
  kable(caption = "Frequency table, educ\n(NA values for vote96 removed",
        col.names = c("educ", "count"),
        format = "html")
```

Based on the graph, the relationship between number of years of education and voting rate appears parabolic, but based on the frequency table it is probably the lack of observations for educ < 6 that is behind the downward slope in the beginning. I assume that the relationship between the variables is linear, as the voting rates for educ >= 6 suggest.

```{r glm2_inc10_plot}
inc10_plot
```

Family income and voting rate seems to be linearly related.

The random component is the Bernoulli distribution:
$Pr(Y_i = y_i | \pi) = \pi_i^{y_i}(1 - \pi_i)^{(1 - y_i)}$

The linear predictor is:
$\eta_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \beta_3X_{3i} + \beta_4X_{4i} + \beta_5X_{5i} + \beta_6X_{6i} + \beta_7X_{7i} + \beta_8X_{2i}^2$

where $X_1$ is mental health, $X_2$ is age, $X_3$ is education level, $X_4$ is race, $X_5$ is gender, $X_6$ is marriage status, and $X_7$ is family income.

The link function is the logit function:
$\pi_i = \frac{e^{\eta_i}}{(1 + e^{\eta_i})}$

  2. **Estimate the model and report your results.**
  
```{r glm2_model}
glm_mult <- glm(vote96 ~ mhealth_sum + age + educ + black + female + married + inc10 + I(age^2), 
                data = mhealth, family = binomial)
summary(glm_mult)
```

  3. **Interpret the results.** 

  
paragraph format
This should include a discussion of your results as if you were reviewing them with fellow computational social scientists. Discuss the results using any or all of log-odds, odds, predicted probabilities, and first differences - choose what makes sense to you and provides the most value to the reader. Use graphs and tables as necessary to support your conclusions.

Using the 0.05 cutoff for statistical signifiance, the predictors with statistically significant estimates are mental health, age, education, and family income.

```{r glm1_log_odds}
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

mhealth_mult <- mhealth %>%
  add_predictions(glm_mult) %>%
  mutate(prob = logit2prob(pred)) %>%

ggplot(data = mhealth_mult, mapping = aes(x = mhealth_sum, y = pred)) + 
  geom_point() + 
  geom_smooth() + 
  ylim(0,1) + 
  labs(title = "Mental health vs. predicted probability of voting for the 1996 election",
       subtitle = "vote96 ~ mhealth_sum + age + educ + black + female + married + inc10 + I(age^2)",
       x = "Mental health index",
       y = "Probability of voting") + 
  theme(plot.subtitle = element_text(face = "italic"))
```


```{r glm1first_difference}
#first differences in probability of voting for increase in mental health index

#1 to 2
fd_12 <- mhealth_grid$prob[mhealth_grid$mhealth_sum == 1] - mhealth_grid$prob[mhealth_grid$mhealth_sum == 2]
#5 to 6
fd_56 <- mhealth_grid$prob[mhealth_grid$mhealth_sum == 5] - mhealth_grid$prob[mhealth_grid$mhealth_sum == 6]
```

#Part 2: Modeling tv consumption

##Estimate a regression model (3 points)

After plotting different predictors against `tvhours`, the non-categorical variables that seem to have a significant effect on the number of hours of tv watched are education, the number of children, and hours of relaxation. I assume from the graphs that the relationships are linear, as while the smoothing lines show curves, they curve only where the data points are relatively sparse. I leave number of children out of the model as it overlaps significantly with education and probably with race. I include some standard demographic dummy variables such as race and gender, as well as voting. I assume an interaction effect between race and education.

```{r}
ggplot(data = gss, mapping = aes(x = educ, y = tvhours)) + 
  geom_point(position = "jitter") + 
  geom_smooth() + 
  labs(title = "Education vs. TV consumption",
       x = "Years of formal schooling",
       y = "Number of hours of TV watched per day")
```

```{r}
ggplot(data = gss, mapping = aes(x = childs, y = tvhours)) + 
  geom_point(position = "jitter") + 
  geom_smooth(method = "lm") + 
  labs(title = "Number of children vs. TV consumption",
       x = "Number of children",
       y = "Number of hours of TV watched per day")
```


```{r}
ggplot(data = gss, mapping = aes(x = hrsrelax, y = tvhours)) + 
  geom_point(position = "jitter") + 
  geom_smooth() + 
  labs(title = "Hours of relaxation vs. TV consumption",
       x = "Hours per day respondent has to relax",
       y = "Number of hours of TV watched per day")
```

```{r}
ggplot(data = gss, mapping = aes(x = childs, y = educ)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(title = "Number of children vs. years of education",
       x = "Number of children",
       y = "Years of formal schooling")
```



1. **Write out the three components of the GLM for your specific model of interest.**

The random component is the Poisson distribution:

$P(Y_i = y_i | \mu) = \frac{\mu^ke^{-y_i}}{y_i!}$

The linear predictor is:

$\eta_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3x_{3i} + \beta_4x_{4i} + \beta_5x_{5i} + \beta_6(x_{1i}*x_{4i})$

where $X_1$ is education, $X_2$ is hours of relaxation, $X_3$ is gender, $X_4$ is race, and $X_5$ is whether the respondent voted in the 2004 election.

The canonical link function for the Poisson distribution is the log function:

$\mu_i = ln(\eta_i)$

  2. **Estimate the model and report your results.**

```{r glmpois_model}
glm_pois <- glm(tvhours ~ hrsrelax + female + educ*black + voted04, 
                data = gss, family = "poisson")
summary(glm_pois)
```

  3. **Interpret the results.**
  
paragraph format. This should include a discussion of your results as if you were reviewing them with fellow computational social scientists. Discuss the results using any or all of log-counts, predicted event counts, and first differences - choose what makes sense to you and provides the most value to the reader. Is the model over or under-dispersed? Use graphs and tables as necessary to support your conclusions.

```{r glm_pois_dispersion}
gss_quasimod <- glm(tvhours ~ hrsrelax + female + educ*black + voted04, family = "quasipoisson", data = gss)
summary(gss_quasimod)
```

The model appears to over-dispersed, but the dispersion parameter is still close to 1; 

```{r session_info, include=FALSE}
devtools::session_info()
```