---
title: 'Perspectives on Computational Modeling: Problem Set #7'
author: "Xingyun Wu"
date: "2017/2/26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(tidyverse)
library(readr)
library(modelr)
library(broom)
library(dplyr)
library(rcfss)
library(knitr)
library(splines)
library(lattice)
library(gam)
```


```{r read-in the datasets, include=FALSE}
bidenData <- read.csv(file="biden.csv")
collegeData <- read.csv(file="College.csv")
```


## Part 1: Sexy Joe Biden (redux)

1. The mean squared error for the linear regression model using the training set is 395.2702.
```{r problem1_1, echo=FALSE}
#define the function of MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

# linear regression
problem1_1 <- lm(biden ~ age + female + dem + rep + educ, data = bidenData)
summary(problem1_1)

# MSE
mse(problem1_1, bidenData)
```

2. The test MSE fo the model using the validation set approach: 399.8303. Compared to the training MSE with the entire dataset, the test MSE is a little bit higher. This means the model fits the training data better, but on the other hand produces more errors when fitting the testing data.
```{r problem1_2, echo=FALSE}
# spit the sample
set.seed(1234)
biden_split <- resample_partition(bidenData, c(test = 0.3, train = 0.7))

# fit the linear regression model
problem1_2 <- lm(biden ~ age + female + dem + rep + educ, data = biden_split$train)
summary(problem1_2)

# MSE
mse(problem1_2, biden_split$test)
```

3. After repeating the validation set approach 100 times, I found the test MSEs are not stable. According to the histogram generated with the 100 test MSEs, they could be widely spread. And everytime I rerun the validation set approach, the distribution of the test MSEs are not exactly the same.
```{r problem1_3, echo=FALSE}
mse_variable <- function(data){
  data_split <- resample_partition(data, c(test = 0.3, train = 0.7))
  data_train <- data_split$train %>%
    tbl_df()
  data_test <- data_split$test %>%
    tbl_df()
  
  model = lm(biden ~ age+female+educ+dem+rep,
                                          data = data_train)

  results <- data_frame(MSE = mse(model, data_test))

  return(results)
}


# build an empty dataframe
data_split <- data.frame(MSE=c(NA))

i <- 0
while(i < 100){
  i <- i + 1;
  temp_mse <- mse_variable(bidenData)
  data_split <- rbind(data_split, temp_mse)
  data_split <- tail(data_split, 100)
}

ggplot(data_split, mapping=aes(x=MSE))+
  geom_histogram(binwidth = 1)+
  labs(title="The distribution of MSEs",
         x="MSE",
         y="Frequency")

mean(data_split$MSE)
```

4. Using the leave-one-out cross-validation (LOOCV) approach, the test MSE is 397.9555. It is close to the test MSEs estimated by the validation set approach. And the test MSE is very stable, which is an advantage over the validation set approach.
```{r problem1_4, echo=FALSE}
loocv_data <- crossv_kfold(bidenData, k = nrow(bidenData))

loocv_models <- map(loocv_data$train, ~ lm(biden ~ age+female+educ+dem+rep, data = .))
loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
mean(loocv_mse)
```

```{r problem1_4_optimal_terms, echo=FALSE}
cv_error <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  loocv_models <- map(loocv_data$train, ~ lm(biden ~ poly(age+female+educ+dem+rep, i), data = .))
  loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
  cv_error[[i]] <- mean(loocv_mse)
}

cv_mse <- data_frame(terms = terms,
           cv_MSE = cv_error)
cv_mse
```

5. Compared to the LOOCV approach, the test MSE for the 10-fold cross-validation approach is lower.
```{r problem1_5, echo=FALSE}
cv10_data <- crossv_kfold(bidenData, k = 10)

cv_error_fold10 <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  cv10_models <- map(cv10_data$train, ~ lm(biden ~ poly(age+female+educ+dem+rep, i), data = .))
  cv10_mse <- map2_dbl(cv10_models, cv10_data$test, mse)
  cv_error_fold10[[i]] <- mean(cv10_mse)
}

cv_error_fold10
```

```{r problem1_5_plot, echo=FALSE}
data_frame(terms = terms,
           loocv = cv_error,
           fold10 = cv_error_fold10) %>%
  gather(method, MSE, loocv:fold10) %>%
  ggplot(aes(terms, MSE, color = method)) +
  geom_line() +
  labs(title = "MSE estimates",
       x = "Degree of Polynomial",
       y = "Mean Squared Error",
       color = "CV Method")
```

6. 
```{r problem1_6, echo=FALSE}
fold10_100 <- data.frame(one=c(NA), two=c(NA), three=c(NA), four=c(NA), five=c(NA))

i <- 0

while (i<100){
  i <- i + 1
  cv10_data <- crossv_kfold(bidenData, k = 10)

    cv_error_fold10 <- vector("numeric", 5)
    terms <- 1:5

    for(i in terms){
      cv10_models <- map(cv10_data$train, ~ lm(biden ~ poly(age+female+educ+dem+rep, i), data = .))
      cv10_mse <- map2_dbl(cv10_models, cv10_data$test, mse)
      cv_error_fold10[[i]] <- mean(cv10_mse)
    }

    fold10_100 <- rbind(fold10_100, cv_error_fold10) 
}

fold10_100
```

7. The estimated parameters are similar between the results of the original model in step 1 is close and the results of the bootstrap.
  The standard errors are also similar. But for 4 out of 5 variables, `age`, `female`, `educ` and `rep`, the standard errors of the bootstrap is slightly larger than the standard errors of the original model. This may due to the bootstrap approach does not rely on any distributional assumptions.
```{r problem1_7_origin, echo=FALSE}
# traditional parameter estimates and standard errors
biden_lm <- lm(biden ~ age+female+educ+dem+rep, data = bidenData)
tidy(biden_lm)
```

```{r problem1_7_boots, echo=FALSE}
# bootstrapped estimates of the parameter estimates and standard errors
biden_boot <- bidenData %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ lm(biden ~ age+female+educ+dem+rep, data = .)),
         coef = map(model, tidy))

biden_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE))
```


## Part 2: College (bivariate)

### 1. No transformation
  The first model is a simple linear regression model, estimating the relationship between instructional expenditure per student and the out-of-state tuition, without transformation. According to the plot below, the simple linear regression without transforamtion does not fit the data vell, although it seems to have captured the general trend of the relationship.
```{r problem2_1_model, echo=FALSE}
sim_linear_mod <- lm(Outstate ~ Expend, data = collegeData)
summary(sim_linear_mod)

ggplot(collegeData, aes(Expend, Outstate)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Linear model for a linear relationship")
```

```{r problem2_1_residuals, echo=FALSE}
sim_linear_pred <- collegeData %>%
  add_predictions(sim_linear_mod) %>%
  add_residuals(sim_linear_mod)

# distribution of residuals
ggplot(sim_linear_pred, aes(resid)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dnorm,
                args = list(mean = mean(sim_linear_pred$resid),
                            sd = sd(sim_linear_pred$resid))) +
  labs(title = "Linear model for a linear relationship",
       x = "Residuals")

```

```{r problem2_1_pred&resid, echo=FALSE}
# predicted vs. residuals
ggplot(sim_linear_pred, aes(pred, resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = "Linear model for a linear relationship",
       x = "Predicted values",
       y = "Residuals")
```

### 2. Polynomial regression
  The second model have the same predictor and response as the previous model, but it applies the polynomial regression approach. According to the results of the model, the parameters for each one of linear, quadratic and cubic terms are statistically significant on the 0.001 significance level. With a better use of information, this model provides a better estimation, compared to the previous model without transformation.
  And the parameter of the intercept becomes smaller and less statistically significant, which means some unexplained variance in the non-transformation model has been explained by this polynomial regression model.
```{r problem2_2_model, echo=FALSE}
# estimate model
problem2_2 <- glm(Outstate ~ I(Expend^1) + I(Expend^2) + I(Expend^3), data = collegeData)
summary(problem2_2)
```
  According to the plot below, this approach fits the data well. Compared to the non-transformation model, more data points are fitted by this model and captured by the 95% confidence interval. This again justify the polynomial regression approach.
```{r problem2_2_estimate&plot, echo=FALSE}
# estimate the predicted values and confidence interval
college_pred <- augment(problem2_2, newdata = data_grid(collegeData, Expend)) %>%
  mutate(pred_low = .fitted - 1.96 * .se.fit,
         pred_high = .fitted + 1.96 * .se.fit)

# plot the curve
ggplot(college_pred, aes(Expend)) +
  geom_point(data = collegeData, aes(Expend, Outstate), alpha = .05) +
  geom_line(aes(y = .fitted)) +
  geom_line(aes(y = pred_low), linetype = 2) +
  geom_line(aes(y = pred_high), linetype = 2) +
  labs(title = "Polynomial regression of Outstate vs. Expend",
       subtitle = "With 95% confidence interval",
       x = "Expend",
       y = "Predicted percent of new students from out-of-state tuition")
```

```{r problem2_2_vcov, echo=FALSE}
vcov(problem2_2) %>%
  kable(caption = "Variance-covariance matrix of Outstate vs. Expend polynomial regression",
        digits = 5)
```


### 3. Regression splines

```{r problem2_3_model, echo=FALSE}
piece_mod1 <- lm(Outstate ~ poly(Expend, 5, raw = TRUE), data = collegeData, subset = Expend < 20000)
piece_mod2 <- lm(Outstate ~ poly(Expend, 5, raw = TRUE), data = collegeData, subset = Expend >= 20000)

summary(piece_mod1)
summary(piece_mod2)
```



## Part 3: College (GAM)

1. The data is splitted into a training set (70%) and a test set(30%).
```{r problem3_1_split}
set.seed(1234)
college_split <- resample_partition(collegeData, c(test = 0.3, train = 0.7))
```


2. The table below shows the results of OLS model on the training data. Each one of the predictors is statistically significant on the 0.001 level. According to the p-value, the predictors explain about 72.32% of variance of the response. The train MSE is 4310543, while the test MSE is 3595260.
```{r problem3_2_OLS, echo=FALSE}
problem3_lm <- lm(Outstate ~ Private+Room.Board+PhD+perc.alumni+Expend+Grad.Rate, data = college_split$train)
summary(problem3_lm)

mse(problem3_2, college_split$train)
mse(problem3_2, college_split$test)
```


3. The table below shows the results of the GAM model. I use the local regression fitting technique for this part, since there may be localized relationships between the predictors and the response.
```{r problem3_3_GAM, echo=FALSE}
problem3_gam <- gam(Outstate ~ Private + lo(Room.Board) + lo(PhD) + lo(perc.alumni) + lo(Expend) + lo(Grad.Rate), data = college_split$train)
summary(problem3_gam)
```
  The plots below show the relationships between each of the predictors and the response `Outstate`.
  According to the plot below, there is a positive relationship between `Room.Board` and `Outstate`. When the room and board costs increase, the out-of-state tuition also increases. But before the room and board costs reach 4,000, the slop is increasing when tthe costs are increasing. After it reach 4,000, the slop gets stable.
```{r problem3_3_plot Room.Board, ehco=FALSE}
# get graphs of each term
problem3_gam_terms <- preplot(problem3_gam, se = TRUE, rug = FALSE)

## Room.Board
data_frame(x = problem3_gam_terms$`lo(Room.Board)`$x,
           y = problem3_gam_terms$`lo(Room.Board)`$y,
           se.fit = problem3_gam_terms$`lo(Room.Board)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       subtitle = "Local regression",
       x = "Room.Board",
       y = expression(f[1](Room.Board)))
```

  According to the plot below, we could see a generally positive relationship between `PhD` and `Outstate`. However, when the percent of falculty with Ph.D.'s is approximately between 30 and 50, the relationship is negative. But this does not influence the overall positive trend.
```{r problem3_3_plot PhD, ehco=FALSE}
## PhD
data_frame(x = problem3_gam_terms$`lo(PhD)`$x,
           y = problem3_gam_terms$`lo(PhD)`$y,
           se.fit = problem3_gam_terms$`lo(PhD)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       subtitle = "Local regression",
       x = "PhD",
       y = expression(f[1](PhD)))
```

  According to the plot below, we could see a positive relationship between `perc.alumni` and `Outstate`. The out-of-state tuition gets increased with the increase of percent of alumni who donate.
```{r problem3_3_plot perc.alumni, ehco=FALSE}
## perc.alumni
data_frame(x = problem3_gam_terms$`lo(perc.alumni)`$x,
           y = problem3_gam_terms$`lo(perc.alumni)`$y,
           se.fit = problem3_gam_terms$`lo(perc.alumni)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       subtitle = "Local regression",
       x = "perc.alumni",
       y = expression(f[1](perc.alumni)))
```

  With `Expend`, we could see that the relationship between `Expend` and `Outstate` is more complex than an OLS relationship. Before `Expend` gets to 30,000, the out-of-state tuition is increased with the increase of instructional expenditure per student. But when `Expend` gets larger than 30,000, the out-of-state tuition gets decreased with the increase of instructional expenditure per student.
```{r problem3_3_plot Expend, ehco=FALSE}
## Expend
data_frame(x = problem3_gam_terms$`lo(Expend)`$x,
           y = problem3_gam_terms$`lo(Expend)`$y,
           se.fit = problem3_gam_terms$`lo(Expend)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       subtitle = "Local regression",
       x = "perc.alumni",
       y = expression(f[1](Expend)))
```

  With `Grad.Rate`, we could see that the relationship between `Grad.Rate` and `Outstate` is more complex than an OLS relationship. Before `Grad.Rate` gets to 80, the out-of-state tuition is increased with the increase of graduation rate. But when `Grad.Rate` gets larger than 80, the out-of-state tuition gets decreased with the increase of graduation rate.
```{r problem3_3_plot Grad.Rate, ehco=FALSE}
## Grad.Rate
data_frame(x = problem3_gam_terms$`lo(Grad.Rate)`$x,
           y = problem3_gam_terms$`lo(Grad.Rate)`$y,
           se.fit = problem3_gam_terms$`lo(Grad.Rate)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       subtitle = "Local regression",
       x = "Grad.Rate",
       y = expression(f[1](Grad.Rate)))
```
  With the plot for `Private`, we could see that being a private university has a substantially positive effect on out-of-state tuition.
```{r problem3_3_plot Private, echo=FALSE}
data_frame(x = problem3_gam_terms$Private$x,
           y = problem3_gam_terms$Private$y,
           se.fit = problem3_gam_terms$Private$se.y) %>%
  unique %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = c('No', 'Yes'), labels = c("Public", "Private"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of out-of-state tuition",
       x = NULL,
       y = expression(f[3](Private)))
```


4. The test MSE for OLS model and the test MSE for GAM model are shown as below.
```{r problem3_4_evaluation}
lin_mse <- mse(problem3_lm, college_split$test)
gam_mse <- mse(problem3_gam, college_split$test)
```


5. According to the plots in part(3), `Expend` and `Grad.Rate` seem to have long-linear relationship with the response.
  For `Expend`, approximately below 30,000, the relationship between `Expend` and `Outstate` is positive. But approximately above 30,000, the relationship between `Expend` and `Outstate` is getting negative. This would not be a linear relationship.
  For `Grad.Rate`, it is similar. Approximately below 80, the relationship between `Grad.Rate` and `Outstate` is positive, but approximately above 80, the relationship between `Grad.Rate` and `Outstate` is negative. This would not be a linear relationship.